# -*- coding: utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt


class MultiLinearRegression(object):
    def __init__(self, dim_in, learning_rate=0.01, max_iter=100, seed=None):
        """
        ä¸€å…ƒçº¿æ€§å›å½’ç±»çš„æ„é€ å‡½æ•°ï¼š
        å‚æ•° å­¦ä¹ ç‡ï¼šlearning_rate
        å‚æ•° æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼šmax_iter
        å‚æ•° seedï¼šäº§ç”Ÿéšæœºæ•°çš„ç§å­
        ä»æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·wçš„åˆå§‹å€¼
        """
        np.random.seed(seed)
        self.lr = learning_rate
        self.max_iter = max_iter
        self.w = np.random.normal(1, 0.1, [dim_in+1, 1]) # w çš„ç»´åº¦ä¸ºè¾“å…¥ç»´åº¦+1
        self.loss_arr = []

    def fit(self, x, y):
        """
        ç±»çš„æ–¹æ³•ï¼šè®­ç»ƒå‡½æ•°
        å‚æ•° è‡ªå˜é‡ï¼šx
        å‚æ•° å› å˜é‡ï¼šy
        è¿”å›æ¯ä¸€æ¬¡è¿­ä»£åçš„æŸå¤±å‡½æ•°
        """
        #é¦–å…ˆåœ¨xçŸ©é˜µåé¢å¢åŠ ä¸€åˆ—1
        x = np.hstack([x, np.ones((x.shape[0], 1))]) #add ones
        for i in range(self.max_iter):
            self.__train_step(x, y)
            y_pred = self.predict(x)
            self.loss_arr.append(self.loss(y, y_pred))

    def __f(self, x, w):
        '''
        ç±»çš„æ–¹æ³•ï¼šè®¡ç®—ä¸€å…ƒçº¿æ€§å›å½’å‡½æ•°åœ¨xå¤„çš„å€¼
        '''
        return np.matmul(x,w)


    def predict(self, x):
        '''
        ç±»çš„æ–¹æ³•ï¼šé¢„æµ‹å‡½æ•°
        å‚æ•°ï¼šè‡ªå˜é‡ï¼šx
        è¿”å›ï¼šå¯¹xçš„å›å½’å€¼
        '''
        y_prd = self.__f(x, self.w)
        return y_prd

    def loss(self, y_true, y_pred):
        '''
        ç±»çš„æ–¹æ³•ï¼šè®¡ç®—æŸå¤±
        å‚æ•° çœŸå®å› å˜é‡ï¼šy_true
        å‚æ•° é¢„æµ‹å› å˜é‡ï¼šy_pred
        è¿”å›ï¼šMSEæŸå¤±
        '''
        return np.mean((y_true - y_pred) ** 2)

#ğŸ/ğ‘µ ğ—^ğ“ (ğ—ğ°âˆ’ğ²)
    def __calc_gradient(self, x, y):
        '''
        ç±»çš„æ–¹æ³•ï¼šåˆ†åˆ«è®¡ç®—å¯¹wå’Œbçš„æ¢¯åº¦
        '''
        N = x.shape[0]
        diff = (np.matmul(x,self.w) - y)
        grad = np.matmul(x.T, diff)
        d_w = (2 * grad) / N

        #print("d_w.shape=", d_w.shape)
        return d_w

    def __train_step(self, x, y):
        '''
        ç±»çš„æ–¹æ³•ï¼šå•æ­¥è¿­ä»£ï¼Œå³ä¸€æ¬¡è¿­ä»£ä¸­å¯¹æ¢¯åº¦è¿›è¡Œæ›´æ–°
        '''
        d_w = self.__calc_gradient(x, y)
        self.w = self.w - self.lr * d_w
        return self.w

# data generation
np.random.seed(272)
data_size = 100

dim_in = 3
dim_out = 1

x = np.random.uniform(low=1.0, high=10.0, size=[data_size, dim_in])
print("x.shape = ", x.shape)
w_true = np.array([[1.5], [-5.], [3.]])
y = np.matmul(x, w_true) + 10 + np.random.normal(loc=0.0, scale=10.0, size=[data_size, dim_out])


# train / test split
shuffled_index = np.random.permutation(data_size)
x = x[shuffled_index, :]
y = y[shuffled_index, :]


split_index = int(data_size * 0.7)
x_train = x[:split_index, :]
y_train = y[:split_index, :]
x_test = x[split_index:, :]
y_test = y[split_index:, :]

# train the liner regression model
regr = MultiLinearRegression(dim_in, learning_rate=0.01, max_iter=100, seed=0)
regr.fit(x_train, y_train)
print(regr.w)
#============================================
x_test_aug = np.hstack([x_test, np.ones((x_test.shape[0], 1))])
y_pred = regr.predict(x_test_aug)
res = y_pred - y_test


# plot the evolution of cost
plt.scatter(np.arange(len(regr.loss_arr)), regr.loss_arr, marker='o', c='green')
plt.show()
