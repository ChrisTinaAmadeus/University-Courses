{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "t.Tensor(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6139, 0.5250, 0.2663, 0.4823],\n",
       "         [0.2710, 0.7316, 0.5744, 0.6101],\n",
       "         [0.8897, 0.3953, 0.8055, 0.4434]],\n",
       "\n",
       "        [[0.3992, 0.5951, 0.1418, 0.9815],\n",
       "         [0.1010, 0.7685, 0.1532, 0.0930],\n",
       "         [0.9436, 0.6559, 0.9900, 0.9952]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成一个随机Tensor\n",
    "x = t.rand(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "2 3 4\n"
     ]
    }
   ],
   "source": [
    "# 查看x的形状\n",
    "print(x.size())\n",
    "\n",
    "print(x.size()[0], x.size()[1], x.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.3572, 0.9911, 0.3879, 1.0980],\n",
       "         [1.1191, 0.9839, 1.1505, 1.4815],\n",
       "         [1.6654, 0.4094, 1.6125, 1.0769]],\n",
       "\n",
       "        [[0.7692, 1.1587, 0.4174, 1.0043],\n",
       "         [0.1080, 0.7859, 0.3691, 0.1333],\n",
       "         [1.2520, 1.5130, 1.5858, 1.5905]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#两个Tensor相加\n",
    "y = t.rand(2, 3, 4)\n",
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-82390817fe76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 注意两个张量对应的维度要匹配\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mz1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# 注意两个张量对应的维度要匹配\n",
    "y1 = t.rand(3, 4, 2)\n",
    "z1 = x + y1\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.6158, 2.4099],\n",
      "         [1.6914, 2.5418],\n",
      "         [1.0842, 2.8695],\n",
      "         [1.0661, 2.9857]],\n",
      "\n",
      "        [[1.9948, 2.1169],\n",
      "         [1.6392, 2.9650],\n",
      "         [1.9776, 2.1294],\n",
      "         [1.5117, 2.4440]],\n",
      "\n",
      "        [[1.8684, 2.7351],\n",
      "         [1.7210, 2.6772],\n",
      "         [1.8509, 2.1567],\n",
      "         [1.3382, 2.0610]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不是所有情况都可以广播\n",
    "y2 = t.rand(2, 1, 2)# 2, 3, 4\n",
    "z2 = x + y1\n",
    "print(z2)\n",
    "z2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6139, 0.5250, 0.2663, 0.4823],\n",
      "         [0.2710, 0.7316, 0.5744, 0.6101],\n",
      "         [0.8897, 0.3953, 0.8055, 0.4434]],\n",
      "\n",
      "        [[0.3992, 0.5951, 0.1418, 0.9815],\n",
      "         [0.1010, 0.7685, 0.1532, 0.0930],\n",
      "         [0.9436, 0.6559, 0.9900, 0.9952]]])\n",
      "tensor([[[1.3572, 0.9911, 0.3879, 1.0980],\n",
      "         [1.1191, 0.9839, 1.1505, 1.4815],\n",
      "         [1.6654, 0.4094, 1.6125, 1.0769]],\n",
      "\n",
      "        [[0.7692, 1.1587, 0.4174, 1.0043],\n",
      "         [0.1080, 0.7859, 0.3691, 0.1333],\n",
      "         [1.2520, 1.5130, 1.5858, 1.5905]]])\n",
      "tensor([[[1.3572, 0.9911, 0.3879, 1.0980],\n",
      "         [1.1191, 0.9839, 1.1505, 1.4815],\n",
      "         [1.6654, 0.4094, 1.6125, 1.0769]],\n",
      "\n",
      "        [[0.7692, 1.1587, 0.4174, 1.0043],\n",
      "         [0.1080, 0.7859, 0.3691, 0.1333],\n",
      "         [1.2520, 1.5130, 1.5858, 1.5905]]])\n",
      "tensor([[[0.6139, 0.5250, 0.2663, 0.4823],\n",
      "         [0.2710, 0.7316, 0.5744, 0.6101],\n",
      "         [0.8897, 0.3953, 0.8055, 0.4434]],\n",
      "\n",
      "        [[0.3992, 0.5951, 0.1418, 0.9815],\n",
      "         [0.1010, 0.7685, 0.1532, 0.0930],\n",
      "         [0.9436, 0.6559, 0.9900, 0.9952]]])\n",
      "tensor([[[1.3572, 0.9911, 0.3879, 1.0980],\n",
      "         [1.1191, 0.9839, 1.1505, 1.4815],\n",
      "         [1.6654, 0.4094, 1.6125, 1.0769]],\n",
      "\n",
      "        [[0.7692, 1.1587, 0.4174, 1.0043],\n",
      "         [0.1080, 0.7859, 0.3691, 0.1333],\n",
      "         [1.2520, 1.5130, 1.5858, 1.5905]]])\n",
      "tensor([[[1.3572, 0.9911, 0.3879, 1.0980],\n",
      "         [1.1191, 0.9839, 1.1505, 1.4815],\n",
      "         [1.6654, 0.4094, 1.6125, 1.0769]],\n",
      "\n",
      "        [[0.7692, 1.1587, 0.4174, 1.0043],\n",
      "         [0.1080, 0.7859, 0.3691, 0.1333],\n",
      "         [1.2520, 1.5130, 1.5858, 1.5905]]])\n"
     ]
    }
   ],
   "source": [
    "#通过函数add实现加法\n",
    "print(x)\n",
    "print(t.add(x, y))\n",
    "\n",
    "print(x.add(y))\n",
    "print(x)\n",
    "\n",
    "print(x.add_(y))## operator_   in_place \n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.1004, 1.4572, 0.5094, 1.7136],\n",
       "         [1.9672, 1.2362, 1.7267, 2.3529],\n",
       "         [2.4412, 0.4234, 2.4195, 1.7103]],\n",
       "\n",
       "        [[1.1393, 1.7223, 0.6929, 1.0271],\n",
       "         [0.1149, 0.8034, 0.5849, 0.1736],\n",
       "         [1.5604, 2.3700, 2.1817, 2.1858]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#另外一个写法，结果输出到提前定义好的Tensor中\n",
    "result = t.Tensor(2, 3, 4)\n",
    "t.add(x, y, out = result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4451, 0.0962, 1.2723, 0.8291],\n",
      "         [1.2611, 0.8535, 1.3659, 1.1903],\n",
      "         [1.3993, 1.2535, 0.6625, 1.1738]],\n",
      "\n",
      "        [[0.9912, 1.3405, 0.9066, 1.2636],\n",
      "         [1.1613, 1.4513, 1.0810, 0.9068],\n",
      "         [0.2541, 1.2024, 1.1832, 1.6327]]])\n",
      "tensor([[[0.4451, 0.0962, 1.2723, 0.8291],\n",
      "         [1.2611, 0.8535, 1.3659, 1.1903],\n",
      "         [1.3993, 1.2535, 0.6625, 1.1738]],\n",
      "\n",
      "        [[0.9912, 1.3405, 0.9066, 1.2636],\n",
      "         [1.1613, 1.4513, 1.0810, 0.9068],\n",
      "         [0.2541, 1.2024, 1.1832, 1.6327]]])\n",
      "tensor([[[0.6774, 0.1715, 2.2705, 0.9805],\n",
      "         [2.2601, 1.3506, 2.3619, 1.4963],\n",
      "         [2.3865, 1.9251, 1.1884, 1.9892]],\n",
      "\n",
      "        [[1.1931, 2.2940, 1.5435, 1.7038],\n",
      "         [2.1163, 2.1989, 1.1915, 1.1962],\n",
      "         [0.4156, 1.9192, 1.4419, 2.4235]]])\n"
     ]
    }
   ],
   "source": [
    "# 函数名带下划线 x.add_(y)会改变x，不带下划线x.add(y) 返回一个新的Tensor，x不改变\n",
    "print(x)\n",
    "x.add(y)\n",
    "print(x)\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2., -2., -2.],\n",
      "        [-2., -2., -2.]])\n"
     ]
    }
   ],
   "source": [
    "a =t.ones(2, 3)\n",
    "b = 3.0 * t.ones(2, 3)\n",
    "c = a - b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d8132db88a45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "d = t.ones(2,2)\n",
    "e = a - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[ 2.,  8.],\n",
      "        [18., 32.],\n",
      "        [50., 72.]])\n",
      "tensor(182.)\n",
      "tensor([[ 10.,  22.,  34.],\n",
      "        [ 22.,  50.,  78.],\n",
      "        [ 34.,  78., 122.]])\n",
      "tensor([[ 70.,  88.],\n",
      "        [ 88., 112.]])\n",
      "torch.Size([3, 2]) torch.Size([3, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e58e4b615c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "a = t.Tensor([[1,2], [3,4], [5, 6]])\n",
    "print(a)\n",
    "\n",
    "#数乘矩阵\n",
    "b = 2.0 * a\n",
    "print(b)\n",
    "\n",
    "#对应点相乘，sum后即为卷积\n",
    "c = a * b\n",
    "print(c)\n",
    "print(c.sum())######\n",
    "\n",
    "# a: 3 * 2, b.t():2 * 3\n",
    "# d: 3 * 3\n",
    "d = a.mm(b.t())\n",
    "print(d)\n",
    "\n",
    "\n",
    "# a.t(): 2 * 3, b.t():3 * 2\n",
    "# e: 2 * 2\n",
    "e = a.t().mm(b)\n",
    "print(e)\n",
    "\n",
    "print(a.size(), b.size())\n",
    "a.mm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5., 11., 17., 23.],\n",
      "        [11., 25., 39., 53.],\n",
      "        [17., 39., 61., 83.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 4x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ad4f550863e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#如果矩阵大小不满足 x: i * n, y:n * j的方式，则出错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 4x2)"
     ]
    }
   ],
   "source": [
    "a = t.Tensor([[1,2], [3,4], [5, 6]]) # 3 * 2\n",
    "b = t.Tensor([[1, 2],[3,4],[5,6],[7,8]]) # 4 * 2\n",
    "\n",
    "c = a.mm(b.t()) # 3 * 4 \n",
    "print(c)\n",
    "\n",
    "\n",
    "#如果矩阵大小不满足 x: i * n, y:n * j的方式，则出错\n",
    "c = a.mm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[1., 2.]])\n",
      "tensor([[ 5.],\n",
      "        [11.],\n",
      "        [17.]])\n",
      "tensor([[ 5., 11., 17.]])\n"
     ]
    }
   ],
   "source": [
    "#A: 3 * 2\n",
    "#x: 1 * 2\n",
    "A = t.Tensor([[1,2], [3,4], [5, 6]]) \n",
    "x = t.Tensor([[1, 2]])\n",
    "print(A)\n",
    "print(x)\n",
    "\n",
    "# A: 3*2; b.t(): 2 * 1\n",
    "# 结果：3 * 1\n",
    "c = A.mm(x.t())\n",
    "print(c)\n",
    "\n",
    "#x: 1 * 2, A.t(): 2 * 3\n",
    "# 结果: 1 * 3\n",
    "d = x.mm(A.t())\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "torch.Size([6])\n",
      "tensor([[1., 2., 3., 4., 5., 6.]])\n",
      "tensor([[1., 2., 3., 4., 5., 6.]])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "a=t.Tensor([[[1,2,3],[4,5,6]]])\n",
    "b=t.Tensor([1,2,3,4,5,6])\n",
    "\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "\n",
    "print(a.view(1,6))\n",
    "print(b.view(1,6))\n",
    "\n",
    "\n",
    "print(a.size())\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3., 4.],\n",
      "         [5., 6., 7., 8.]]])\n",
      "torch.Size([1, 2, 4])\n",
      "---------------------------------\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "---------------------------------\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "a=t.Tensor([[[1,2,3, 4],[5,6,7,8]]])\n",
    "print(a)\n",
    "print(a.size())\n",
    "print('---------------------------------')\n",
    "print(a.view(4,2))\n",
    "print('---------------------------------')\n",
    "print(a.view(2, 2 ,2))\n",
    "print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.]]])\n"
     ]
    }
   ],
   "source": [
    "#因为是视图，所以发生了修改\n",
    "c = a.view(2, 2 ,2)\n",
    "c[0,0,0] = -1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4457, 0.2665, 0.5816, 0.7822],\n",
      "         [0.1515, 0.6697, 0.7244, 0.8637],\n",
      "         [0.2203, 0.3986, 0.6592, 0.2722]],\n",
      "\n",
      "        [[0.8131, 0.5206, 0.7370, 0.1588],\n",
      "         [0.7189, 0.0197, 0.4226, 0.5261],\n",
      "         [0.5736, 0.4399, 0.5709, 0.3668]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4457, 0.2665, 0.5816, 0.7822])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor的选取操作与numpy类似\n",
    "x = t.rand(2,3,4)\n",
    "print(x)\n",
    "\n",
    "x[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4457, 0.8131])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4457, 0.1515, 0.2203],\n",
       "        [0.8131, 0.7189, 0.5736]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6296, 0.2885],\n",
       "         [0.4374, 0.4617]],\n",
       "\n",
       "        [[0.4501, 0.2051],\n",
       "         [0.1279, 0.0084]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1:3, 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2636, 0.6643, 0.3245],\n",
      "        [0.1376, 0.1650, 0.1767]])\n",
      "tensor([[0.0598, 0.4083, 0.4071, 0.3481],\n",
      "        [0.3681, 0.5724, 0.2711, 0.5821]])\n",
      "tensor([[0.2636, 0.6643, 0.3245, 0.0598, 0.4083, 0.4071, 0.3481],\n",
      "        [0.1376, 0.1650, 0.1767, 0.3681, 0.5724, 0.2711, 0.5821]])\n"
     ]
    }
   ],
   "source": [
    "x = t.rand(2,3)\n",
    "print(x)\n",
    "y = t.rand(2,4)\n",
    "print(y)\n",
    "z = t.cat((x,y), dim=1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before squeezing\n",
      " tensor([[[[0.1797, 0.4137, 0.0305],\n",
      "          [0.7431, 0.0329, 0.4445]]]]) torch.Size([1, 1, 2, 3])\n",
      "after squeezing\n",
      " tensor([[0.1797, 0.4137, 0.0305],\n",
      "        [0.7431, 0.0329, 0.4445]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = t.rand(1,1,2,3)\n",
    "print(\"before squeezing\\n\", x, x.size())\n",
    "x = t.squeeze(x)\n",
    "print(\"after squeezing\\n\", x, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyTorch Tensor与Numpy之间的转换\n",
    "#tensor->numpy\n",
    "a = t.ones(2, 3, 4)#Tensor\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy --> Tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = t.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# 上述转换中，Tensor和Numpy共享内容，所以修改一个另一个也随之改变\n",
    "\n",
    "print(a)\n",
    "b.add_(1) #注意是下划线add_\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not A\n",
      "tensor([[0.5322, 0.7454, 1.4098],\n",
      "        [0.6225, 1.6208, 1.1990]])\n"
     ]
    }
   ],
   "source": [
    "x = t.rand(2,3)\n",
    "y = t.rand(2,3)\n",
    "\n",
    "#支持GPU，将x和y都转移到GPU中进行运算\n",
    "# 在大规模数据复杂Tensor运算时，具有优势；\n",
    "# 小规模数据时由于具有数据转移开销，CPU会更快一点\n",
    "\n",
    "#本机器不支持GPU，不会运行以下代码\n",
    "if t.cuda.is_available():\n",
    "    x = x.cuda() #将张量x转移到GPU中\n",
    "    y = y.cuda() #将张量y转移到GPU中\n",
    "    z = x + y #在GPU中运算x + y\n",
    "else:\n",
    "    print(\"Not A\")\n",
    "    z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#如果不进行判断，x.cuda()语句在没有GPU环境的情况下出错\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;66;03m#将张量x转移到GPU中\u001b[39;00m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;66;03m#将张量y转移到GPU中\u001b[39;00m\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m+\u001b[39m y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#如果不进行判断，x.cuda()语句在没有GPU环境的情况下出错\n",
    "x = x.cuda() #将张量x转移到GPU中\n",
    "y = y.cuda() #将张量y转移到GPU中\n",
    "x + y #在GPU中运算x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# 求函数 y= x^2 在x=3时的导数: y'(3) = dy/dx|x=3\n",
    "\n",
    "x=t.tensor(3.0,requires_grad=True)\n",
    "y=x.mul(x)\n",
    "\n",
    "#判断x,y是否是可以求导的\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)\n",
    "\n",
    "#求导，通过backward函数来实现\n",
    "y.backward()\n",
    "\n",
    "#查看导数，也即所谓的梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 练习\n",
    "# 求y = max(0, x)在x=1和x=-1处的导数\n",
    "\n",
    "\n",
    "#定义x=1.0 和y=max(0, x)\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.max(x, t.zeros(1,1))\n",
    "\n",
    "#调用backward并打印导数\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# 改变x后需要重新执行y和backward\n",
    "x = t.tensor(-1.0, requires_grad=True)\n",
    "y = t.max(x, t.zeros(1,1))\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.)\n",
      "tensor(-1.)\n",
      "tensor(-0.2500)\n"
     ]
    }
   ],
   "source": [
    "# 求y = 1/x\n",
    "x = t.tensor(0.5, requires_grad=True)\n",
    "y = t.reciprocal(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.reciprocal(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(2.0, requires_grad=True)\n",
    "y = t.reciprocal(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.4142)\n",
      "tensor(-0.5000)\n",
      "tensor(-0.1768)\n"
     ]
    }
   ],
   "source": [
    "# 求y = 1/\\sqrt(x)\n",
    "x = t.tensor(0.5, requires_grad=True)\n",
    "y = t.reciprocal( t.sqrt(x))\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.reciprocal( t.sqrt(x))\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(2.0, requires_grad=True)\n",
    "y = t.reciprocal( t.sqrt(x))\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 求y = 1/\\sqrt(x)\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "g = t.sqrt(x)\n",
    "y = t.reciprocal(g)\n",
    "\n",
    "g.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "# f(x,y) = x^2 + 2 * y^2 + xy\n",
    "# 在 (1.0, 1.0)处的导数\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.tensor(1.0, requires_grad=True)\n",
    "\n",
    "f = x.pow(2) + t.tensor(2.0).mul(y.pow(2)) + x.mul(y)\n",
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0064)\n",
      "tensor(0.0471)\n",
      "tensor(0.9465)\n"
     ]
    }
   ],
   "source": [
    "# f(x,y, z) = ln(e^x + e^y + e^z)\n",
    "# 在 (0.0, 2.0, 5.0)处的导数\n",
    "x = t.tensor(0.0, requires_grad=True)\n",
    "y = t.tensor(2.0, requires_grad=True)\n",
    "z = t.tensor(5.0, requires_grad=True)\n",
    "\n",
    "f = t.log(t.exp(x) + t.exp(y) + t.exp(z))\n",
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.]], requires_grad=True) tensor([[1., 1.],\n",
      "        [1., 2.]]) tensor([[1.],\n",
      "        [0.]])\n",
      "\n",
      "tensor([[14.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[ 7.],\n",
      "        [10.]])\n"
     ]
    }
   ],
   "source": [
    "# 注意：把x和b定义为一个2*1的矩阵，x.mm()是矩阵乘法\n",
    "import torch as t\n",
    "x = t.tensor([[1.0], [2.0]], requires_grad=True)\n",
    "\n",
    "A = t.tensor([[1.0, 1.0], \n",
    "              [1.0, 2.0]])\n",
    "b = t.tensor([[1.0], [0.0]])\n",
    "print(x, A, b)\n",
    "print()\n",
    "\n",
    "f = x.t().mm(A).mm(x) + x.t().mm(b)\n",
    "print(f)\n",
    "print()\n",
    "f.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.]], requires_grad=True) tensor([[3.],\n",
      "        [4.]], requires_grad=True) tensor([[1., 1.],\n",
      "        [1., 2.]]) tensor([[1.],\n",
      "        [0.]])\n",
      "\n",
      "tensor([[30.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[ 8.],\n",
      "        [11.]])\n",
      "tensor([[3.],\n",
      "        [5.]])\n"
     ]
    }
   ],
   "source": [
    "x = t.tensor([[1.0], [2.0]], requires_grad=True)\n",
    "y = t.tensor([[3.0], [4.0]], requires_grad=True)\n",
    "\n",
    "A = t.tensor([[1.0, 1.0], \n",
    "              [1.0, 2.0]])\n",
    "b = t.tensor([[1.0], [0.0]])\n",
    "print(x, y, A, b)\n",
    "print()\n",
    "\n",
    "f = x.t().mm(A).mm(y) + x.t().mm(b)\n",
    "print(f)\n",
    "print()\n",
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.]]) tensor([[3.],\n",
      "        [4.]]) tensor([[ 2., 12.],\n",
      "        [ 1.,  2.]], requires_grad=True) tensor([[1.],\n",
      "        [0.]])\n",
      "\n",
      "tensor([[77.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[3., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# f(A) = x^T A y + x^T b\n",
    "x = t.tensor([[1.0], [2.0]])\n",
    "y = t.tensor([[3.0], [4.0]])\n",
    "\n",
    "A = t.tensor([[2.0, 12.0], \n",
    "              [1.0, 2.0]], requires_grad=True)\n",
    "b = t.tensor([[1.0], [0.0]])\n",
    "print(x, y, A, b)\n",
    "print()\n",
    "\n",
    "f = x.t().mm(A).mm(y) + x.t().mm(b)\n",
    "print(f)\n",
    "print()\n",
    "f.backward()\n",
    "#print(x.grad)\n",
    "#print(y.grad)\n",
    "print(A.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
